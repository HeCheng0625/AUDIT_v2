{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL, PNDMScheduler, DDPMScheduler\n",
    "from transformers import T5EncoderModel, T5TokenizerFast\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = torch.device(\"cuda:0\")\n",
    "num_inference_steps = 100\n",
    "guidance_scale= 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/blob/v-yuancwang/AudioEditingModel/Diffusion_SG/checkpoint-350000\"\n",
    "unet_path = \"/blob/v-yuancwang/AUDITPLUS/AUDIT_G_0/checkpoint-450000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(unet_path)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_path, subfolder=\"tokenizer\")\n",
    "text_encoder = T5EncoderModel.from_pretrained(model_path, subfolder=\"text_encoder\")\n",
    "\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "print(scheduler.num_train_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 tensor(0.8500) tensor(12.0000)\n",
      "1000 tensor(0.8500) tensor(5348.9927)\n"
     ]
    }
   ],
   "source": [
    "betas = scheduler.betas.clone().detach()\n",
    "num_train_timesteps = scheduler.num_train_timesteps\n",
    "betas = betas * num_train_timesteps\n",
    "print(len(betas), betas[0], betas[-1])\n",
    "betas_cumsum = torch.cumsum(betas, dim=0)\n",
    "print(len(betas_cumsum), betas_cumsum[0], betas_cumsum[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 tensor(999) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "num_inference_steps = 1000\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "print(len(scheduler.timesteps), scheduler.timesteps[0], scheduler.timesteps[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta_cum_t(betas, timesteps, num_train_timesteps=1000):\n",
    "    dt = 1 / num_train_timesteps\n",
    "    betas_cumsum = torch.cumsum(betas, dim=0)\n",
    "    return betas_cumsum[timesteps] * dt\n",
    "\n",
    "def get_mean(betas, timesteps, num_train_timesteps=1000):\n",
    "    return torch.exp(-0.5 * get_beta_cum_t(betas, timesteps, num_train_timesteps))\n",
    "\n",
    "def get_variance(betas, timesteps, num_train_timesteps=1000):\n",
    "    return 1.0 - torch.exp(-get_beta_cum_t(betas, timesteps, num_train_timesteps))\n",
    "\n",
    "def get_logp_xt(pred_noise, betas, timesteps, num_train_timesteps=1000):\n",
    "    variance = get_variance(betas, timesteps, num_train_timesteps).to(device=pred_noise.device, dtype=pred_noise.dtype)\n",
    "    print(pred_noise.shape)\n",
    "    return - pred_noise / (variance ** 0.5).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "def ode_step(xt, pred_noise, betas, total_inference_steps, inference_steps, num_train_timesteps=1000):\n",
    "    dt = - 1 / total_inference_steps\n",
    "    logp_xt = get_logp_xt(pred_noise, betas, inference_steps, num_train_timesteps)\n",
    "    print(logp_xt.shape)\n",
    "    return xt + dt * (-0.5) * (xt + logp_xt) * betas[inference_steps].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).to(device=pred_noise.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([329, 751, 936, 562, 406, 971, 949, 733, 447, 246, 299, 166, 698, 502,\n",
      "        251, 295, 844,  88, 219, 175, 803,  64, 398, 315, 481,  76, 843, 469,\n",
      "         32, 254, 255, 573, 567, 969, 658,  10, 766, 621, 247,   6, 446, 102,\n",
      "        103, 438, 695,  80, 996, 943, 400, 136, 469, 158, 935, 945, 895, 617,\n",
      "        911, 496, 719, 479, 895,  99, 828, 662, 213, 187, 891, 409, 373, 869,\n",
      "        202,  58, 851, 839, 207, 491, 198, 217, 109, 244, 484, 918, 214, 864,\n",
      "        448, 669, 661, 340, 204, 413, 855, 659,   7,  76, 822,  64, 220, 388,\n",
      "        395, 241])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 4, 10, 78])\n",
      "torch.Size([100, 4, 10, 78])\n",
      "torch.Size([100, 4, 10, 78])\n"
     ]
    }
   ],
   "source": [
    "inference_steps = torch.randint(0, 1000, (100,))\n",
    "print(inference_steps)\n",
    "inference_steps = inference_steps.long()\n",
    "xt = torch.randn(100, 4, 10, 78)\n",
    "pred_noise = torch.randn(100, 4, 10, 78)\n",
    "print(get_mean(betas, inference_steps + 1).shape)\n",
    "print(get_variance(betas, inference_steps + 1).shape)\n",
    "print(ode_step(xt, pred_noise, betas, 1000, inference_steps).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = scheduler.timesteps\n",
    "print(timesteps[::5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"baby crying\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [text]\n",
    "text_input = tokenizer(prompt, max_length=tokenizer.model_max_length, truncation=True, padding=\"do_not_pad\", return_tensors=\"pt\")\n",
    "text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * 1, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
    "print(text_embeddings.shape)\n",
    "print(uncond_embeddings.shape)\n",
    "text_embeddings = text_embeddings\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = torch.randn((1, 4, 10, 78)).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inference_step in tqdm(timesteps[::5]):\n",
    "    with torch.no_grad():\n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "        pred_noise = unet(latent_model_input, inference_step, encoder_hidden_states=text_embeddings).sample\n",
    "        noise_pred_uncond, noise_pred_text = pred_noise.chunk(2)\n",
    "        pred_noise = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "        latents = ode_step(latents, pred_noise, betas, 200, inference_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_out = latents\n",
    "with torch.no_grad():\n",
    "    res = vae.decode(latents_out).sample\n",
    "res = res.cpu().numpy()[0,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(\"/home/v-yuancwang/AUDIT_v2/test_mel\", text+\".npy\"), res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio(\"/home/v-yuancwang/AUDIT_v2/test_wav/play guitar.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
